from sys import size_of, argv
from testing import assert_equal
from gpu.host import DeviceContext

# ANCHOR: naive_matmul
from gpu import thread_idx, block_idx, block_dim, barrier
from gpu.memory import AddressSpace
from layout import Layout, LayoutTensor


comptime TPB = 3
comptime SIZE = 2
comptime BLOCKS_PER_GRID = (1, 1)
comptime THREADS_PER_BLOCK = (TPB, TPB)
comptime dtype = DType.float32
comptime layout = Layout.row_major(SIZE, SIZE)


fn naive_matmul[
    layout: Layout, size: UInt
](
    output: LayoutTensor[dtype, layout, MutAnyOrigin],
    a: LayoutTensor[dtype, layout, ImmutAnyOrigin],
    b: LayoutTensor[dtype, layout, ImmutAnyOrigin],
):
    row = block_dim.y * block_idx.y + thread_idx.y
    col = block_dim.x * block_idx.x + thread_idx.x
    # FILL ME IN (roughly 6 lines)

    if row < size and col < size:
        var c:output.element_type = 0.0

        @parameter
        for i in range(size):
            c += a[row, i] * b[i,col]

        output[row,col] = c


# ANCHOR_END: naive_matmul


# ANCHOR: single_block_matmul
fn single_block_matmul[
    layout: Layout, size: UInt
](
    output: LayoutTensor[dtype, layout, MutAnyOrigin],
    a: LayoutTensor[dtype, layout, ImmutAnyOrigin],
    b: LayoutTensor[dtype, layout, ImmutAnyOrigin],
):
    row = block_dim.y * block_idx.y + thread_idx.y
    col = block_dim.x * block_idx.x + thread_idx.x
    local_row = thread_idx.y
    local_col = thread_idx.x
    # FILL ME IN (roughly 12 lines)

    shared_a = LayoutTensor[
        dtype,
        Layout.row_major(TPB, TPB),
        MutAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    shared_b = LayoutTensor[
        dtype,
        Layout.row_major(TPB, TPB),
        MutAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    if row < size and col < size:
        shared_a[local_row,local_col] = a[row, col]
        shared_b[local_row,local_col] = b[row, col]

    barrier()

    if row < size and col < size:
        var c:output.element_type = 0.0

        @parameter
        for i in range(size):
            c += shared_a[local_row, i] * shared_b[i, local_col]

        output[row,col] = c


# ANCHOR_END: single_block_matmul

# ANCHOR: matmul_tiled
comptime SIZE_TILED = 9
comptime BLOCKS_PER_GRID_TILED = (3, 3)  # each block convers 3x3 elements
comptime THREADS_PER_BLOCK_TILED = (TPB, TPB)
comptime layout_tiled = Layout.row_major(SIZE_TILED, SIZE_TILED)


fn matmul_tiled[
    layout: Layout, size: UInt
](
    output: LayoutTensor[dtype, layout_tiled, MutAnyOrigin],
    a: LayoutTensor[dtype, layout_tiled, ImmutAnyOrigin],
    b: LayoutTensor[dtype, layout_tiled, ImmutAnyOrigin],
):
    # indices in the current block (tile 3x3)
    local_row = thread_idx.y
    local_col = thread_idx.x
    # indices over tile boundries (current row or col of entire 9x9 matrix)
    tiled_row = block_idx.y * TPB + thread_idx.y
    tiled_col = block_idx.x * TPB + thread_idx.x
    # FILL ME IN (roughly 20 lines)

    shared_a = LayoutTensor[
        dtype,
        Layout.row_major(TPB, TPB), # [3x3]
        MutAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    shared_b = LayoutTensor[
        dtype,
        Layout.row_major(TPB, TPB), # [3x3]
        MutAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    # each thread computes a single entry in c
    # c is also a register value for each individual thread meaning accumulating this value is fast!
    var c:output.element_type = 0.0

    comptime num_tiles = (size + TPB - 1) // TPB #(9 + 3 - 1) // 3 = 11 // 3 = 3
    # => each entry in c requires the results from 3 tiles.

    @parameter
    for tile in range(num_tiles):

        # fill shared memory with current tiles (rows from a and cols from b)
        # each tile is associated with a single block
        global_idx_col = tile * TPB + local_col
        global_idx_row = tile * TPB + local_row

        if tiled_row < size and global_idx_col < size:
            shared_a[local_row, local_col] = a[tiled_row, global_idx_col]

        if tiled_col < size and global_idx_row < size:
            # note how shared_b has same shape as shared_a so we have coalesced memory access!!
            shared_b[local_row, local_col] = b[global_idx_row, tiled_col]

        barrier()
        # => at this point we have 3x3 shared memory tiles for input a and 3x3 shared memory tiles for input b.

        if tiled_row < size and tiled_col < size:

            # accumulate results from all tiles involved from a and b to obtain current output c
            @parameter
            for k in range(min(TPB, size - tile*TPB)): # min(3, 9 - {0,1,2}*3)
                c += shared_a[local_row, k] * shared_b[k, local_col]

        barrier()


    if tiled_row < size and tiled_col < size:
        output[tiled_row, tiled_col] = c

# ANCHOR_END: matmul_tiled


def main():
    with DeviceContext() as ctx:
        if len(argv()) != 2 or argv()[1] not in [
            "--naive",
            "--single-block",
            "--tiled",
        ]:
            raise Error(
                "Expected one argument: '--naive', '--single-block', or"
                " '--tiled'"
            )
        size = SIZE_TILED if argv()[1] == "--tiled" else SIZE
        out = ctx.enqueue_create_buffer[dtype](size * size)
        out.enqueue_fill(0)
        inp1 = ctx.enqueue_create_buffer[dtype](size * size)
        inp1.enqueue_fill(0)
        inp2 = ctx.enqueue_create_buffer[dtype](size * size)
        inp2.enqueue_fill(0)
        expected = ctx.enqueue_create_host_buffer[dtype](size * size)
        expected.enqueue_fill(0)

        with inp1.map_to_host() as inp1_host, inp2.map_to_host() as inp2_host:
            for row in range(size):
                for col in range(size):
                    val = row * size + col
                    # row major: placing elements row by row
                    inp1_host[row * size + col] = val
                    inp2_host[row * size + col] = Float32(2.0) * val

            # inp1 @ inp2.T
            for i in range(size):
                for j in range(size):
                    for k in range(size):
                        expected[i * size + j] += (
                            inp1_host[i * size + k] * inp2_host[k * size + j]
                        )

        out_tensor = LayoutTensor[dtype, layout, MutAnyOrigin](out)
        a_tensor = LayoutTensor[dtype, layout, ImmutAnyOrigin](inp1)
        b_tensor = LayoutTensor[dtype, layout, ImmutAnyOrigin](inp2)

        if argv()[1] == "--naive":
            comptime kernel = naive_matmul[layout, UInt(SIZE)]
            ctx.enqueue_function_checked[kernel, kernel](
                out_tensor,
                a_tensor,
                b_tensor,
                grid_dim=BLOCKS_PER_GRID,
                block_dim=THREADS_PER_BLOCK,
            )
        elif argv()[1] == "--single-block":
            comptime kernel = single_block_matmul[layout, UInt(SIZE)]
            ctx.enqueue_function_checked[kernel, kernel](
                out_tensor,
                a_tensor,
                b_tensor,
                grid_dim=BLOCKS_PER_GRID,
                block_dim=THREADS_PER_BLOCK,
            )
        elif argv()[1] == "--tiled":
            # Need to update the layout of the tensors to the tiled layout
            out_tensor_tiled = LayoutTensor[dtype, layout_tiled, MutAnyOrigin](
                out
            )
            a_tensor_tiled = LayoutTensor[dtype, layout_tiled, ImmutAnyOrigin](
                inp1
            )
            b_tensor_tiled = LayoutTensor[dtype, layout_tiled, ImmutAnyOrigin](
                inp2
            )

            comptime kernel = matmul_tiled[layout_tiled, UInt(SIZE_TILED)]
            ctx.enqueue_function_checked[kernel, kernel](
                out_tensor_tiled,
                a_tensor_tiled,
                b_tensor_tiled,
                grid_dim=BLOCKS_PER_GRID_TILED,
                block_dim=THREADS_PER_BLOCK_TILED,
            )

        ctx.synchronize()

        with out.map_to_host() as out_host:
            print("out:", out_host)
            print("expected:", expected)
            for col in range(size):
                for row in range(size):
                    assert_equal(
                        out_host[col * size + row], expected[col * size + row]
                    )
